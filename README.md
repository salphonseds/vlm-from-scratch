# Vision-Language Model (VLM) from Scratch

A learning project to build a Vision-Language Model with conversational capabilities using pre-trained encoders and custom alignment layers.

## ğŸ¯ Project Goals

- Understand multi-modal AI architecture deeply
- Build an image captioning model
- Add conversational AI capabilities
- Train on AWS SageMaker (g5.12xlarge)

## ğŸ—ï¸ Architecture

```
Image â†’ Vision Encoder (CLIP/ViT) â†’ Projection Layer â†’ Language Model (GPT-2) â†’ Response
                                          â†“
                                   Conversation Context
```

**Components:**
- **Vision Encoder**: Pre-trained CLIP ViT-B/16 (87M params, frozen)
- **Projection Layer**: MLP to align vision & language embeddings (6M params, trainable)
- **Language Model**: Pre-trained GPT-2 (124M params, fine-tuned)

**Total**: 217M parameters, 60% trainable

## ğŸ“ Project Structure

```
vlm-from-scratch/
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ 01_data_exploration.ipynb      # Explore COCO dataset
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ dataset.py                 # PyTorch Dataset classes
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ vision_encoder.py          # CLIP/ViT encoder
â”‚   â”‚   â”œâ”€â”€ projection.py              # Alignment layer
â”‚   â”‚   â”œâ”€â”€ vlm.py                     # Complete VLM model
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ train.py                       # Main training script
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ base_config.yaml               # Base configuration
â”‚   â””â”€â”€ training_config.yaml           # Training parameters
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ download_data.sh               # Download datasets
â”œâ”€â”€ requirements.txt                    # Python dependencies
â”œâ”€â”€ .gitignore                         # Git ignore rules
â””â”€â”€ README.md                          # This file
```

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

### 2. Download Dataset

```bash
chmod +x scripts/download_data.sh
bash scripts/download_data.sh
```

### 3. Explore Data

```bash
jupyter notebook notebooks/01_data_exploration.ipynb
```

### 4. Train Model

```bash
# Single GPU
python src/train.py --config configs/training_config.yaml

# Multi-GPU (4x A10G on g5.12xlarge)
torchrun --nproc_per_node=4 src/train.py --config configs/training_config.yaml
```

## ğŸ“Š Datasets

### Phase 1: COCO Captions
- **Size**: 330K images with 5 captions each
- **Purpose**: Learn basic image-to-text alignment
- **Download**: Automatic via scripts

### Phase 2: LLaVA-Instruct-150K (Future)
- **Size**: 150K instruction-following examples
- **Purpose**: Add conversational capabilities

## ğŸ–¥ï¸ Hardware Requirements

**Minimum:**
- GPU: 16GB VRAM (e.g., V100, A10G)
- RAM: 32GB
- Storage: 50GB

**Recommended (Used in this project):**
- **AWS g5.12xlarge**: 4x A10G GPUs (24GB each)
- RAM: 192GB
- Storage: 100GB

## ğŸ“ˆ Expected Results

| Epoch | Train Loss | Val Loss | Time | Captions |
|-------|------------|----------|------|----------|
| 0 | 8.5 | 8.2 | - | Random |
| 5 | 3.0 | 2.8 | ~3h | Basic phrases |
| 10 | 2.2 | 2.3 | ~6h | Good sentences âœ… |

## ğŸ“ Learning Path

1. **Week 1-2**: Data preparation & model architecture
2. **Week 3-4**: Training & optimization
3. **Week 5-6**: Conversational capabilities
4. **Week 7**: Deployment & demo

## ğŸ“ Key Learnings

- Vision-language alignment techniques
- Multi-modal attention mechanisms
- Efficient fine-tuning strategies
- Multi-GPU distributed training
- Transfer learning with frozen encoders

## ğŸ¤ Contributing

This is a learning project, but feedback and suggestions are welcome!

## ğŸ“„ License

MIT License - Feel free to use this for learning!

## ğŸ™ Acknowledgments

- COCO Dataset team
- Hugging Face for pre-trained models
- LLaVA project for instruction datasets
- AWS SageMaker for compute resources

---

**Status**: ğŸš§ Active Development | **Last Updated**: November 2025